@inproceedings{julian-parperm,
  title = {Sequential Random Permutation, List Contraction and Tree Contraction
           are Highly Parallel},
  author = {Julian Shun and Yan Gu and Guy E. Blelloch and Jeremy T. Fineman and
            Phillip B. Gibbons},
  booktitle = {ACM-SIAM Symposium on Discrete Algorithms},
  year = {2015},
  url = {https://api.semanticscholar.org/CorpusID:1687316},
}

@article{blelloch-detreserve,
  author = {Blelloch, Guy E. and Fineman, Jeremy T. and Gibbons, Phillip B. and
            Shun, Julian},
  title = {Internally deterministic parallel algorithms can be fast},
  year = {2012},
  issue_date = {August 2012},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {47},
  number = {8},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/2370036.2145840},
  doi = {10.1145/2370036.2145840},
  abstract = {The virtues of deterministic parallelism have been argued for
              decades and many forms of deterministic parallelism have been
              described and analyzed. Here we are concerned with one of the
              strongest forms, requiring that for any input there is a unique
              dependence graph representing a trace of the computation annotated
              with every operation and value. This has been referred to as
              internal determinism, and implies a sequential semantics---i.e.,
              considering any sequential traversal of the dependence graph is
              sufficient for analyzing the correctness of the code. In addition
              to returning deterministic results, internal determinism has many
              advantages including ease of reasoning about the code, ease of
              verifying correctness, ease of debugging, ease of defining
              invariants, ease of defining good coverage for testing, and ease of
              formally, informally and experimentally reasoning about
              performance. On the other hand one needs to consider the possible
              downsides of determinism, which might include making algorithms (i)
              more complicated, unnatural or special purpose and/or (ii) slower
              or less scalable.In this paper we study the effectiveness of this
              strong form of determinism through a broad set of benchmark
              problems. Our main contribution is to demonstrate that for this
              wide body of problems, there exist efficient internally
              deterministic algorithms, and moreover that these algorithms are
              natural to reason about and not complicated to code. We leverage an
              approach to determinism suggested by Steele (1990), which is to use
              nested parallelism with commutative operations. Our algorithms
              apply several diverse programming paradigms that fit within the
              model including (i) a strict functional style (no shared state
              among concurrent operations), (ii) an approach we refer to as
              deterministic reservations, and (iii) the use of commutative,
              linearizable operations on data structures. We describe algorithms
              for the benchmark problems that use these deterministic approaches
              and present performance results on a 32-core machine. Perhaps
              surprisingly, for all problems, our internally deterministic
              algorithms achieve good speedup and good performance even relative
              to prior nondeterministic solutions.},
  journal = {SIGPLAN Not.},
  month = {feb},
  pages = {181–192},
  numpages = {12},
  keywords = {commutative operations, deterministic parallelism, geometry
              algorithms, graph algorithms, parallel algorithms, parallel
              programming, sorting, string processing},
}

@inproceedings{blelloch-detreserve-proceedings,
  author = {Blelloch, Guy E. and Fineman, Jeremy T. and Gibbons, Phillip B. and
            Shun, Julian},
  title = {Internally deterministic parallel algorithms can be fast},
  year = {2012},
  isbn = {9781450311601},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2145816.2145840},
  doi = {10.1145/2145816.2145840},
  abstract = {The virtues of deterministic parallelism have been argued for
              decades and many forms of deterministic parallelism have been
              described and analyzed. Here we are concerned with one of the
              strongest forms, requiring that for any input there is a unique
              dependence graph representing a trace of the computation annotated
              with every operation and value. This has been referred to as
              internal determinism, and implies a sequential semantics---i.e.,
              considering any sequential traversal of the dependence graph is
              sufficient for analyzing the correctness of the code. In addition
              to returning deterministic results, internal determinism has many
              advantages including ease of reasoning about the code, ease of
              verifying correctness, ease of debugging, ease of defining
              invariants, ease of defining good coverage for testing, and ease of
              formally, informally and experimentally reasoning about
              performance. On the other hand one needs to consider the possible
              downsides of determinism, which might include making algorithms (i)
              more complicated, unnatural or special purpose and/or (ii) slower
              or less scalable.In this paper we study the effectiveness of this
              strong form of determinism through a broad set of benchmark
              problems. Our main contribution is to demonstrate that for this
              wide body of problems, there exist efficient internally
              deterministic algorithms, and moreover that these algorithms are
              natural to reason about and not complicated to code. We leverage an
              approach to determinism suggested by Steele (1990), which is to use
              nested parallelism with commutative operations. Our algorithms
              apply several diverse programming paradigms that fit within the
              model including (i) a strict functional style (no shared state
              among concurrent operations), (ii) an approach we refer to as
              deterministic reservations, and (iii) the use of commutative,
              linearizable operations on data structures. We describe algorithms
              for the benchmark problems that use these deterministic approaches
              and present performance results on a 32-core machine. Perhaps
              surprisingly, for all problems, our internally deterministic
              algorithms achieve good speedup and good performance even relative
              to prior nondeterministic solutions.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and
               Practice of Parallel Programming},
  pages = {181–192},
  numpages = {12},
  keywords = {commutative operations, deterministic parallelism, geometry
              algorithms, graph algorithms, parallel algorithms, parallel
              programming, sorting, string processing},
  location = {New Orleans, Louisiana, USA},
  series = {PPoPP '12},
}





